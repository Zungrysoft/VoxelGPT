{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import random\n",
    "from datetime import datetime\n",
    "import json\n",
    "import training\n",
    "import color\n",
    "import vector as vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# Network Structure\n",
    "CONTEXT_SIZE = 128      # How many other voxels are considered for a training example\n",
    "EMBEDDING_SIZE = 520    # Dimensionality of the voxel embedding vector\n",
    "STACKED_LAYERS = 1      # How many times the network structure repeats itself\n",
    "ATTENTION_HEADS = 10    # Number of heads in each multi-headed attention mechanism\n",
    "\n",
    "# Training Hyperparameters\n",
    "CHECK_RADIUS = 7        # How far away voxels can be to be part of a training example\n",
    "CENTER_FOCUS = 0.3      # How much to focus on picking voxels close to the center of the cube. Must be between 0 and 1.\n",
    "LEARNING_RATE = 1e-3\n",
    "TRAINING_EXAMPLES = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palette has 256 colors\n"
     ]
    }
   ],
   "source": [
    "# Load voxel palette\n",
    "# The output is a 255-dimensional vector of probabilities for different colors\n",
    "# Which 255 colors can be generated is decided by the palette file\n",
    "\n",
    "# Index 0 is reserved as 'undecided' voxel\n",
    "# Index 1 is reserved as 'air' voxel\n",
    "# Index 2-255 are colors. So there are 254 possible colors.\n",
    "with open('data/palette.json', 'r') as json_file:\n",
    "    raw_palette = json.load(json_file)['colors']\n",
    "    palette = color.expand_palette(raw_palette)\n",
    "    palette_size = len(palette)\n",
    "\n",
    "print(f\"Palette has {palette_size} colors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input (InputLayer)             [(None, 128, 520)]   0           []                               \n",
      "                                                                                                  \n",
      " normalization_start_a (LayerNo  (None, 128, 520)    1040        ['input[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_0 (MultiH  (None, 128, 520)    10832120    ['normalization_start_a[0][0]',  \n",
      " eadAttention)                                                    'normalization_start_a[0][0]']  \n",
      "                                                                                                  \n",
      " residual_connection_0a (Add)   (None, 128, 520)     0           ['normalization_start_a[0][0]',  \n",
      "                                                                  'multi_head_attention_0[0][0]'] \n",
      "                                                                                                  \n",
      " normalization_0a (LayerNormali  (None, 128, 520)    1040        ['residual_connection_0a[0][0]'] \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " feedforward_0 (Dense)          (None, 128, 520)     270920      ['normalization_0a[0][0]']       \n",
      "                                                                                                  \n",
      " relu_0 (LeakyReLU)             (None, 128, 520)     0           ['feedforward_0[0][0]']          \n",
      "                                                                                                  \n",
      " residual_connection_0b (Add)   (None, 128, 520)     0           ['normalization_0a[0][0]',       \n",
      "                                                                  'relu_0[0][0]']                 \n",
      "                                                                                                  \n",
      " normalization_0b (LayerNormali  (None, 128, 520)    1040        ['residual_connection_0b[0][0]'] \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " input_next_pos (InputLayer)    [(None, 128, 520)]   0           []                               \n",
      "                                                                                                  \n",
      " concatenate_next_pos (Concaten  (None, 128, 1040)   0           ['normalization_0b[0][0]',       \n",
      " ate)                                                             'input_next_pos[0][0]']         \n",
      "                                                                                                  \n",
      " feedforward_final (Dense)      (None, 128, 255)     265455      ['concatenate_next_pos[0][0]']   \n",
      "                                                                                                  \n",
      " softmax (Softmax)              (None, 128, 255)     0           ['feedforward_final[0][0]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,371,615\n",
      "Trainable params: 11,371,615\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "def main_model():\n",
    "    input = keras.Input(shape=(CONTEXT_SIZE, EMBEDDING_SIZE,), name='input')\n",
    "    input_next_pos = keras.Input(shape=(CONTEXT_SIZE, EMBEDDING_SIZE,), name='input_next_pos')\n",
    "\n",
    "    # Normalization\n",
    "    x = keras.layers.LayerNormalization(name=f'normalization_start_a')(input)\n",
    "\n",
    "    # Stacked layers\n",
    "    for i in range(STACKED_LAYERS):\n",
    "        # Multi-headed attention\n",
    "        fx = keras.layers.MultiHeadAttention(\n",
    "            num_heads=ATTENTION_HEADS,\n",
    "            key_dim=EMBEDDING_SIZE,\n",
    "            name=f'multi_head_attention_{i}',\n",
    "        )(x, x, use_causal_mask=True)\n",
    "\n",
    "        # Residual connection\n",
    "        x = keras.layers.Add(name=f'residual_connection_{i}a')([x,fx])\n",
    "\n",
    "        # Normalization\n",
    "        x = keras.layers.LayerNormalization(name=f'normalization_{i}a')(x)\n",
    "\n",
    "        # Feedforward\n",
    "        fx = keras.layers.Dense(EMBEDDING_SIZE, name=f'feedforward_{i}')(x)\n",
    "        fx = keras.layers.LeakyReLU(name=f'relu_{i}')(fx)\n",
    "\n",
    "        # Residual connection\n",
    "        x = keras.layers.Add(name=f'residual_connection_{i}b')([x,fx])\n",
    "\n",
    "        # Normalization\n",
    "        x = keras.layers.LayerNormalization(name=f'normalization_{i}b')(x)\n",
    "    \n",
    "    # Concatenate with next_pos input\n",
    "    x = keras.layers.Concatenate(axis=2, name='concatenate_next_pos')([x,input_next_pos])\n",
    "\n",
    "    # Final feedforward layer\n",
    "    # Output size should be palette_size-1, since we don't want it to be able to choose \"undecided\"\n",
    "    x = keras.layers.Dense(palette_size-1, name='feedforward_final')(x)\n",
    "\n",
    "    # Softmax\n",
    "    x = keras.layers.Softmax(name='softmax')(x)\n",
    "    \n",
    "    # Build and return model\n",
    "    return keras.Model(inputs=[input, input_next_pos], outputs=x)\n",
    "\n",
    "model = main_model()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss and optimizer\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 training examples created.\n",
      "[((3, 3, 0), 231), ((4, 3, 0), 246), ((0, 4, 0), 246), ((1, 4, 0), 231), ((2, 4, 0), 246), ((3, 4, 0), 246), ((4, 4, 0), 231), ((0, 0, 1), 246), ((1, 0, 1), 246), ((2, 0, 1), 252), ((3, 0, 1), 1), ((4, 0, 1), 1), ((0, 1, 1), 1), ((1, 1, 1), 1), ((2, 1, 1), 1), ((3, 1, 1), 1), ((4, 1, 1), 1), ((0, 2, 1), 1), ((1, 2, 1), 1), ((2, 2, 1), 1), ((3, 2, 1), 1), ((4, 2, 1), 1), ((0, 3, 1), 1), ((1, 3, 1), 1), ((2, 3, 1), 1), ((3, 3, 1), 1), ((4, 3, 1), 1), ((0, 4, 1), 1), ((1, 4, 1), 1), ((2, 4, 1), 1), ((3, 4, 1), 1), ((4, 4, 1), 1), ((0, 0, 2), 1), ((1, 0, 2), 1), ((2, 0, 2), 1), ((3, 0, 2), 1), ((4, 0, 2), 1), ((0, 1, 2), 1), ((1, 1, 2), 1), ((2, 1, 2), 1), ((3, 1, 2), 1), ((4, 1, 2), 1), ((0, 2, 2), 1), ((1, 2, 2), 1), ((2, 2, 2), 1), ((3, 2, 2), 1), ((4, 2, 2), 1), ((0, 3, 2), 1), ((1, 3, 2), 1), ((2, 3, 2), 1), ((3, 3, 2), 1), ((4, 3, 2), 1), ((0, 4, 2), 1), ((1, 4, 2), 1), ((2, 4, 2), 1), ((3, 4, 2), 1), ((4, 4, 2), 1), ((0, 0, 3), 1), ((1, 0, 3), 1), ((2, 0, 3), 1), ((3, 0, 3), 1), ((4, 0, 3), 1), ((0, 1, 3), 1), ((1, 1, 3), 1), ((2, 1, 3), 1), ((3, 1, 3), 1), ((4, 1, 3), 1), ((0, 2, 3), 1), ((1, 2, 3), 1), ((2, 2, 3), 1), ((3, 2, 3), 1), ((4, 2, 3), 1), ((0, 3, 3), 1), ((1, 3, 3), 1), ((2, 3, 3), 1), ((3, 3, 3), 1), ((4, 3, 3), 1), ((0, 4, 3), 1), ((1, 4, 3), 1), ((2, 4, 3), 1), ((3, 4, 3), 1), ((4, 4, 3), 1), ((0, 0, 4), 1), ((1, 0, 4), 1), ((2, 0, 4), 1), ((3, 0, 4), 1), ((4, 0, 4), 1), ((0, 1, 4), 1), ((1, 1, 4), 1), ((2, 1, 4), 1), ((3, 1, 4), 1), ((4, 1, 4), 1), ((0, 2, 4), 1), ((1, 2, 4), 1), ((2, 2, 4), 1), ((3, 2, 4), 1), ((4, 2, 4), 1), ((0, 3, 4), 1), ((1, 3, 4), 1), ((2, 3, 4), 1), ((3, 3, 4), 1), ((4, 3, 4), 1), ((0, 4, 4), 1), ((1, 4, 4), 1), ((2, 4, 4), 1), ((3, 4, 4), 1), ((4, 4, 4), 1), ((0, 0, 5), 1), ((1, 0, 5), 1), ((2, 0, 5), 1), ((3, 0, 5), 1), ((4, 0, 5), 1), ((0, 1, 5), 1), ((1, 1, 5), 1), ((2, 1, 5), 1), ((3, 1, 5), 1), ((4, 1, 5), 1), ((0, 2, 5), 1), ((1, 2, 5), 1), ((2, 2, 5), 1), ((3, 2, 5), 1), ((4, 2, 5), 1), ((0, 3, 5), 1), ((1, 3, 5), 1), ((2, 3, 5), 1), ((3, 3, 5), 1), ((4, 3, 5), 1), ((0, 4, 5), 1), ((1, 4, 5), 1)]\n"
     ]
    }
   ],
   "source": [
    "# Design training examples\n",
    "training_examples = training.generate_training_examples(TRAINING_EXAMPLES, CONTEXT_SIZE)\n",
    "\n",
    "print(f\"{len(training_examples)} training examples created.\")\n",
    "\n",
    "print(random.choice(training_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_training_input(example):\n",
    "    inputEntry = []\n",
    "\n",
    "    # Encode context vector\n",
    "    for index, voxel in enumerate(example):\n",
    "        if len(inputEntry) < CONTEXT_SIZE+1:\n",
    "            inputEntry.append(training.embed(index, voxel[0], voxel[1], palette, EMBEDDING_SIZE))\n",
    "\n",
    "    # Pad remainder of context with zeros\n",
    "    if len(inputEntry) < CONTEXT_SIZE+1:\n",
    "        zero_elem = [0,] * EMBEDDING_SIZE\n",
    "        inputEntry += [zero_elem,] * ((CONTEXT_SIZE+1) - len(inputEntry))\n",
    "\n",
    "    return inputEntry\n",
    "\n",
    "def encode_training_input_next_pos(example, last_pos):\n",
    "    inputEntry = []\n",
    "\n",
    "    # Encode context vector\n",
    "    for index in range(len(example)):\n",
    "        pos = example[index][0]\n",
    "        if index < len(example)-1:\n",
    "            next_pos = example[index+1][0]\n",
    "        else:\n",
    "            next_pos = last_pos\n",
    "        if len(inputEntry) < CONTEXT_SIZE+1:\n",
    "            inputEntry.append(training.embed_nptest(index, vec.subtract(next_pos, pos), -1, palette, EMBEDDING_SIZE))\n",
    "\n",
    "    # Pad remainder of context with zeros\n",
    "    if len(inputEntry) < CONTEXT_SIZE+1:\n",
    "        zero_elem = [0,] * EMBEDDING_SIZE\n",
    "        inputEntry += [zero_elem,] * ((CONTEXT_SIZE+1) - len(inputEntry))\n",
    "    \n",
    "    return inputEntry\n",
    "\n",
    "def encode_training_output(example):\n",
    "    outputEntry = []\n",
    "    for voxel in example:\n",
    "        outputEntry.append(training.encode_one_hot(voxel[1], palette_size))\n",
    "    return outputEntry\n",
    "\n",
    "# Reformat training examples into tensor format\n",
    "def encode_training_examples(training_examples):\n",
    "    training_inputs = []\n",
    "    training_inputs_next_pos = []\n",
    "    training_outputs = []\n",
    "    for example in training_examples:\n",
    "        input_entry = encode_training_input(example)\n",
    "        input_entry_next_pos = encode_training_input_next_pos(example, (0, 0, 0))\n",
    "        output_entry = encode_training_output(example)\n",
    "        \n",
    "        # Shift the output\n",
    "        input_entry.pop(-1)\n",
    "        input_entry_next_pos.pop(-1)\n",
    "        output_entry.pop(0)\n",
    "\n",
    "        # Push to training example list\n",
    "        training_inputs.append(input_entry)\n",
    "        training_inputs_next_pos.append(input_entry_next_pos)\n",
    "        training_outputs.append(output_entry)\n",
    "\n",
    "    training_input_tensor = tf.Variable(training_inputs, tf.float64)\n",
    "    training_input_next_pos_tensor = tf.Variable(training_inputs_next_pos, tf.float64)\n",
    "    training_output_tensor = tf.Variable(training_outputs, tf.float64)\n",
    "\n",
    "    return training_input_tensor, training_input_next_pos_tensor, training_output_tensor\n",
    "\n",
    "training_input_tensor, training_input_next_pos_tensor, training_output_tensor = encode_training_examples(training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(training_input_next_pos_tensor[12][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch training data\n",
    "dataset_input = tf.data.Dataset.from_tensor_slices(training_input_tensor)\n",
    "dataset_input_next_pos = tf.data.Dataset.from_tensor_slices(training_input_next_pos_tensor)\n",
    "dataset_output = tf.data.Dataset.from_tensor_slices(training_output_tensor)\n",
    "\n",
    "training_input_batched = dataset_input.batch(128)\n",
    "training_input_next_pos_batched = dataset_input_next_pos.batch(128)\n",
    "training_output_batched = dataset_output.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct an example sculpture using the model's current progress\n",
    "# TODO: Move this to a separate file so we can do multithreading and other improvements\n",
    "def build_sculpture(count, base=None, temperature=1.0):\n",
    "    # If a base wasn't specified, create one\n",
    "    color = int(random.random() * 254) + 1\n",
    "    voxels = {}\n",
    "    start_pos = (0, 0, 0)\n",
    "    # start_pos = (int(random.random() * training.SIZE[0]), int(random.random() * training.SIZE[1]), int(random.random() * training.SIZE[2]))\n",
    "    voxels[vec.ttos(start_pos)] = color\n",
    "    \n",
    "    # Build context vector for the sculpture\n",
    "    context = [(start_pos, color)]\n",
    "\n",
    "    for i in range(count-1):\n",
    "        # Determine where the next voxel will go\n",
    "        # TODO: Encode this data into the model somehow\n",
    "        next_pos = training.pick_next_voxel(voxels, context)\n",
    "\n",
    "        # Get the output from the model\n",
    "        input_data = encode_training_input(context)\n",
    "        input_data.pop(-1)\n",
    "        input_tensor = tf.Variable(input_data, tf.float64)\n",
    "        input_tensor = tf.reshape(input_tensor, [1, -1, EMBEDDING_SIZE])\n",
    "\n",
    "        input_next_pos_data = encode_training_input_next_pos(context, next_pos)\n",
    "        input_next_pos_data.pop(-1)\n",
    "        input_next_pos_tensor = tf.Variable(input_next_pos_data, tf.float64)\n",
    "        input_next_pos_tensor = tf.reshape(input_tensor, [1, -1, EMBEDDING_SIZE])\n",
    "\n",
    "        output = model([input_tensor, input_next_pos_tensor], training=False)\n",
    "        output_probabilities = output[0][len(context)-1]\n",
    "\n",
    "        # Pick which voxel to generate based on output probabilities\n",
    "        # TODO: Implement temperature\n",
    "        choice = random.random()\n",
    "        chosen_voxel = 1\n",
    "        for i in range(len(output_probabilities)):\n",
    "            choice -= output_probabilities[i]\n",
    "            if choice < 0:\n",
    "                chosen_voxel = i+1\n",
    "                break\n",
    "        # chosen_voxel = 1\n",
    "        # best = 0\n",
    "        # for i in range(len(output_probabilities)):\n",
    "        #     if output_probabilities[i] > best:\n",
    "        #         best = output_probabilities[i]\n",
    "        #         chosen_voxel = i+1\n",
    "        \n",
    "        # Build the voxel\n",
    "        voxels[vec.ttos(next_pos)] = chosen_voxel\n",
    "        context.append((next_pos, chosen_voxel))\n",
    "        if len(context) > CONTEXT_SIZE:\n",
    "            context = training.remove_farthest(context, next_pos)\n",
    "        \n",
    "    return voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for saving sculptures to json\n",
    "def save_sculpture(sculpture, filename):\n",
    "    json_data = {\n",
    "        \"size\": {\n",
    "            \"x\": training.SIZE[0],\n",
    "            \"y\": training.SIZE[1],\n",
    "            \"z\": training.SIZE[2],\n",
    "        },\n",
    "        \"voxels\": sculpture,\n",
    "    }\n",
    "    with open(filename, 'w') as output_file:\n",
    "        json.dump(json_data, output_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 1\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 3\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 7\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 13\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 21\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 32\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 47\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 68\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 96\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 134\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 182\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 245\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 328\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 430\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 566\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 749\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 985\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 1305\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 1730\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 2257\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 2952\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 3872\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 5049\n",
      "Building example sculpture...\n",
      "Done\n",
      "Epoch 6509\n",
      "Building example sculpture...\n",
      "Done\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\dev\\VoxelGPT\\run_train.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/dev/VoxelGPT/run_train.ipynb#X13sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m       \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDone\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/dev/VoxelGPT/run_train.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m       example_time \u001b[39m=\u001b[39m (datetime\u001b[39m.\u001b[39mnow()\u001b[39m-\u001b[39mtime_started)\u001b[39m.\u001b[39mtotal_seconds() \u001b[39m*\u001b[39m \u001b[39m1.3\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/dev/VoxelGPT/run_train.ipynb#X13sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m train(\u001b[39m10000000\u001b[39;49m)\n",
      "\u001b[1;32md:\\dev\\VoxelGPT\\run_train.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/dev/VoxelGPT/run_train.ipynb#X13sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/dev/VoxelGPT/run_train.ipynb#X13sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m   \u001b[39m# Minibatches\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/dev/VoxelGPT/run_train.ipynb#X13sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m   \u001b[39mfor\u001b[39;00m (batch_input, batch_input_next_pos, batch_output) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(training_input_batched, training_input_next_pos_batched, training_output_batched):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/dev/VoxelGPT/run_train.ipynb#X13sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     train_step(batch_input, batch_input_next_pos, batch_output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/dev/VoxelGPT/run_train.ipynb#X13sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m   \u001b[39m# Print status\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/dev/VoxelGPT/run_train.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m   \u001b[39m# print(f\"Completed Epoch {epoch}\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/dev/VoxelGPT/run_train.ipynb#X13sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/dev/VoxelGPT/run_train.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m   \u001b[39m# Check if we should output an example sculpture\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/dev/VoxelGPT/run_train.ipynb#X13sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m   \u001b[39mif\u001b[39;00m (datetime\u001b[39m.\u001b[39mnow()\u001b[39m-\u001b[39mtime_started)\u001b[39m.\u001b[39mtotal_seconds() \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m example_time:\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training step\n",
    "@tf.function\n",
    "def train_step(input_data, input_next_pos_data, output_data):\n",
    "    # Set up tape\n",
    "    with tf.GradientTape() as tape:\n",
    "      output = model([input_data, input_next_pos_data], training=True)\n",
    "\n",
    "      loss = loss_function(output_data, output)\n",
    "\n",
    "      gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "      optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "# Training loop\n",
    "def train(epochs):\n",
    "  time_started = datetime.now()\n",
    "  example_time = 3\n",
    "  sculptures_made = 0\n",
    "\n",
    "  # Epochs\n",
    "  for epoch in range(epochs):\n",
    "    # Minibatches\n",
    "    for (batch_input, batch_input_next_pos, batch_output) in zip(training_input_batched, training_input_next_pos_batched, training_output_batched):\n",
    "      train_step(batch_input, batch_input_next_pos, batch_output)\n",
    "\n",
    "    # Print status\n",
    "    # print(f\"Completed Epoch {epoch}\")\n",
    "\n",
    "    # Check if we should output an example sculpture\n",
    "    if (datetime.now()-time_started).total_seconds() >= example_time:\n",
    "      print(f\"Epoch {epoch}\")\n",
    "      print(\"Building example sculpture...\")\n",
    "      sculpture = build_sculpture(training.SIZE[0] * training.SIZE[1] * training.SIZE[2])\n",
    "      # sculpture = build_sculpture(300)\n",
    "      sculptures_made += 1\n",
    "      sculpture_filename = f\"examples/json/example_{(time_started-datetime.utcfromtimestamp(0)).total_seconds()}_{sculptures_made}.json\"\n",
    "      save_sculpture(sculpture, sculpture_filename)\n",
    "      print(\"Done\")\n",
    "      example_time = (datetime.now()-time_started).total_seconds() * 1.3\n",
    "\n",
    "train(10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sorpok.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
