{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from datetime import datetime\n",
    "import math\n",
    "import json\n",
    "import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# Network Structure\n",
    "CONTEXT_SIZE = 8        # How many other voxels are considered for a training example\n",
    "EMBEDDING_SIZE = 64     # Dimensionality of the voxel embedding vector\n",
    "STACKED_LAYERS = 2      # How many times the network structure repeats itself\n",
    "ATTENTION_HEADS = 2     # Number of heads in each multi-headed attention mechanism\n",
    "\n",
    "# Training Hyperparameters\n",
    "CHECK_RADIUS = 7        # How far away voxels can be to be part of a training example\n",
    "CENTER_FOCUS = 0.3      # How much to focus on picking voxels close to the center of the cube. Must be between 0 and 1.\n",
    "LEARNING_RATE = 1e-4\n",
    "TRAINING_EXAMPLES = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palette has 256 colors\n"
     ]
    }
   ],
   "source": [
    "# Load voxel palette\n",
    "# The output is a 255-dimensional vector of probabilities for different colors\n",
    "# Which 255 colors can be generated is decided by the palette file\n",
    "\n",
    "# Index 0 is reserved as 'undecided' voxel\n",
    "# Index 1 is reserved as 'air' voxel\n",
    "# Index 2-255 are colors. So there are 254 possible colors.\n",
    "with open('data/palette.json', 'r') as json_file:\n",
    "    palette = json.load(json_file)['colors']\n",
    "    palette_size = len(palette)\n",
    "\n",
    "print(f\"Palette has {palette_size} colors\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_layer (InputLayer)       [(None, 8, 64)]      0           []                               \n",
      "                                                                                                  \n",
      " feedforward_start (Dense)      (None, 8, 64)        4160        ['input_layer[0][0]']            \n",
      "                                                                                                  \n",
      " multi_head_attention_0 (MultiH  (None, 8, 64)       33216       ['feedforward_start[0][0]',      \n",
      " eadAttention)                                                    'feedforward_start[0][0]']      \n",
      "                                                                                                  \n",
      " normalization_0a (LayerNormali  (None, 8, 64)       128         ['multi_head_attention_0[0][0]'] \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " residual_connection_0a (Add)   (None, 8, 64)        0           ['feedforward_start[0][0]',      \n",
      "                                                                  'normalization_0a[0][0]']       \n",
      "                                                                                                  \n",
      " feedforward_0 (Dense)          (None, 8, 64)        4160        ['residual_connection_0a[0][0]'] \n",
      "                                                                                                  \n",
      " relu_0 (LeakyReLU)             (None, 8, 64)        0           ['feedforward_0[0][0]']          \n",
      "                                                                                                  \n",
      " normalization_0b (LayerNormali  (None, 8, 64)       128         ['relu_0[0][0]']                 \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " residual_connection_0b (Add)   (None, 8, 64)        0           ['residual_connection_0a[0][0]', \n",
      "                                                                  'normalization_0b[0][0]']       \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 8, 64)       33216       ['residual_connection_0b[0][0]', \n",
      " eadAttention)                                                    'residual_connection_0b[0][0]'] \n",
      "                                                                                                  \n",
      " normalization_1a (LayerNormali  (None, 8, 64)       128         ['multi_head_attention_1[0][0]'] \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " residual_connection_1a (Add)   (None, 8, 64)        0           ['residual_connection_0b[0][0]', \n",
      "                                                                  'normalization_1a[0][0]']       \n",
      "                                                                                                  \n",
      " feedforward_1 (Dense)          (None, 8, 64)        4160        ['residual_connection_1a[0][0]'] \n",
      "                                                                                                  \n",
      " relu_1 (LeakyReLU)             (None, 8, 64)        0           ['feedforward_1[0][0]']          \n",
      "                                                                                                  \n",
      " normalization_1b (LayerNormali  (None, 8, 64)       128         ['relu_1[0][0]']                 \n",
      " zation)                                                                                          \n",
      "                                                                                                  \n",
      " residual_connection_1b (Add)   (None, 8, 64)        0           ['residual_connection_1a[0][0]', \n",
      "                                                                  'normalization_1b[0][0]']       \n",
      "                                                                                                  \n",
      " feedforward_final (Dense)      (None, 8, 255)       16575       ['residual_connection_1b[0][0]'] \n",
      "                                                                                                  \n",
      " softmax (Softmax)              (None, 8, 255)       0           ['feedforward_final[0][0]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 95,999\n",
      "Trainable params: 95,999\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "def main_model():\n",
    "    input = keras.Input(shape=(CONTEXT_SIZE, EMBEDDING_SIZE,), name='input_layer')\n",
    "\n",
    "    x = keras.layers.Dense(EMBEDDING_SIZE, name=f'feedforward_start')(input)\n",
    "\n",
    "    # Stacked layers\n",
    "    for i in range(STACKED_LAYERS):\n",
    "        # Multi-headed attention\n",
    "        fx = keras.layers.MultiHeadAttention(\n",
    "            num_heads=ATTENTION_HEADS,\n",
    "            key_dim=EMBEDDING_SIZE,\n",
    "            name=f'multi_head_attention_{i}',\n",
    "        )(x, x, use_causal_mask=True)\n",
    "\n",
    "        # Normalization\n",
    "        fx = keras.layers.LayerNormalization(name=f'normalization_{i}a')(fx)\n",
    "\n",
    "        # Residual connection\n",
    "        x = keras.layers.Add(name=f'residual_connection_{i}a')([x,fx])\n",
    "\n",
    "        # Feedforward\n",
    "        fx = keras.layers.Dense(EMBEDDING_SIZE, name=f'feedforward_{i}')(x)\n",
    "        fx = keras.layers.LeakyReLU(name=f'relu_{i}')(fx)\n",
    "\n",
    "        # Normalization\n",
    "        fx = keras.layers.LayerNormalization(name=f'normalization_{i}b')(fx)\n",
    "\n",
    "        # Residual connection\n",
    "        x = keras.layers.Add(name=f'residual_connection_{i}b')([x,fx])\n",
    "    \n",
    "    # Final feedforward layer\n",
    "    # Output size should be palette_size-1, since we don't want it to be able to choose \"undecided\"\n",
    "    x = keras.layers.Dense(palette_size-1, name='feedforward_final')(x)\n",
    "\n",
    "    # Softmax\n",
    "    x = keras.layers.Softmax(name='softmax')(x)\n",
    "    \n",
    "    # Build and return model\n",
    "    return keras.Model(inputs=input, outputs=x)\n",
    "\n",
    "model = main_model()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss and optimizer\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 training examples created.\n"
     ]
    }
   ],
   "source": [
    "# Design training examples\n",
    "training_examples = training.generate_training_examples(TRAINING_EXAMPLES, CONTEXT_SIZE)\n",
    "\n",
    "print(f\"{len(training_examples)} training examples created.\")\n",
    "\n",
    "def encode_training_input(example):\n",
    "    inputEntry = []\n",
    "    for voxel in example:\n",
    "        inputEntry.append(training.embed(voxel[0], voxel[1], palette, EMBEDDING_SIZE))\n",
    "    if len(inputEntry) < CONTEXT_SIZE:\n",
    "        zero_elem = [0,] * EMBEDDING_SIZE\n",
    "        inputEntry += [zero_elem,] * (CONTEXT_SIZE - len(inputEntry))\n",
    "\n",
    "    return inputEntry\n",
    "\n",
    "def encode_training_input_mask(example):\n",
    "    inputEntry = []\n",
    "    for voxel in example:\n",
    "        inputEntry.append(True)\n",
    "    if len(inputEntry) < CONTEXT_SIZE:\n",
    "        zero_elem = [0,] * EMBEDDING_SIZE\n",
    "        inputEntry += [zero_elem,] * (CONTEXT_SIZE - len(inputEntry))\n",
    "\n",
    "    return inputEntry\n",
    "\n",
    "def encode_training_output(example):\n",
    "    outputEntry = []\n",
    "    for voxel in example:\n",
    "        outputEntry.append(training.encode_one_hot(voxel[1], palette_size))\n",
    "    return outputEntry\n",
    "\n",
    "# Reformat training examples into tensor format\n",
    "def encode_training_examples(training_examples):\n",
    "    training_inputs = []\n",
    "    training_outputs = []\n",
    "    for example in training_examples:\n",
    "        inputEntry = encode_training_input(example)\n",
    "        outputEntry = encode_training_output(example)\n",
    "        \n",
    "        # Shift the output\n",
    "        inputEntry.pop(0)\n",
    "        outputEntry.pop(-1)\n",
    "\n",
    "        # Push to training example list\n",
    "        training_inputs.append(inputEntry)\n",
    "        training_outputs.append(outputEntry)\n",
    "\n",
    "    training_input_tensor = tf.Variable(training_inputs, tf.float64)\n",
    "    training_output_tensor = tf.Variable(training_outputs, tf.float64)\n",
    "\n",
    "    return training_input_tensor, training_output_tensor\n",
    "\n",
    "training_input_tensor, training_output_tensor = encode_training_examples(training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct an example sculpture using the model's current progress\n",
    "# TODO: Move this to a separate file so we can do multithreading and other improvements\n",
    "def build_sculpture(count, base=None, temperature=1.0):\n",
    "    # If a base wasn't specified, create one\n",
    "    voxels = {}\n",
    "    voxels['63,63,63'] = int(random.random() * 254) + 1\n",
    "    \n",
    "    # Build context vector for the sculpture\n",
    "    context = [((63, 63, 63), voxels['63,63,63'])]\n",
    "\n",
    "    for i in range(count):\n",
    "        # Determine where the next voxel will go\n",
    "        # TODO: Encode this data into the model somehow\n",
    "        next_pos = training.pick_next_voxel(voxels, context)\n",
    "\n",
    "        # Get the output from the model\n",
    "        input_data = encode_training_input(context)\n",
    "        input_tensor = tf.Variable(input_data, tf.float64)\n",
    "        input_tensor = tf.reshape(input_tensor, [1, -1, EMBEDDING_SIZE])\n",
    "        output = model(input_tensor, training=False)\n",
    "        output_probabilities = output[0][len(context)-1]\n",
    "\n",
    "        # Pick which voxel to generate based on output probabilities\n",
    "        # TODO: Implement temperature\n",
    "        choice = random.random()\n",
    "        chosen_voxel = 1\n",
    "        for i in range(len(output_probabilities)):\n",
    "            choice -= output_probabilities[i]\n",
    "            if choice < 0:\n",
    "                chosen_voxel = i+1\n",
    "                break\n",
    "        \n",
    "        # Build the voxel\n",
    "        voxels[training.ttos(next_pos)] = chosen_voxel\n",
    "        context.append((next_pos, chosen_voxel))\n",
    "        if len(context) > CONTEXT_SIZE:\n",
    "            context.pop(0)\n",
    "        \n",
    "    return voxels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for saving sculptures to json\n",
    "def save_sculpture(sculpture, filename):\n",
    "    json_data = {\n",
    "        \"size\": {\n",
    "            \"x\": 128,\n",
    "            \"y\": 128,\n",
    "            \"z\": 128,\n",
    "        },\n",
    "        \"voxels\": sculpture,\n",
    "    }\n",
    "    with open(filename, 'w') as output_file:\n",
    "        json.dump(json_data, output_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building example sculpture.\n",
      "Building example sculpture.\n",
      "Building example sculpture.\n",
      "Building example sculpture.\n",
      "Building example sculpture.\n",
      "Building example sculpture.\n",
      "Building example sculpture.\n",
      "Building example sculpture.\n",
      "Building example sculpture.\n",
      "Building example sculpture.\n",
      "Building example sculpture.\n",
      "Building example sculpture.\n",
      "Building example sculpture.\n",
      "Building example sculpture.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 41\u001b[0m\n\u001b[0;32m     36\u001b[0m       sculpture_filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mexamples/json/example_\u001b[39m\u001b[39m{\u001b[39;00m(time_started\u001b[39m-\u001b[39mdatetime\u001b[39m.\u001b[39mutcfromtimestamp(\u001b[39m0\u001b[39m))\u001b[39m.\u001b[39mtotal_seconds()\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00msculptures_made\u001b[39m}\u001b[39;00m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     37\u001b[0m       save_sculpture(sculpture, sculpture_filename)\n\u001b[1;32m---> 41\u001b[0m train(\u001b[39m1000000\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[9], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epochs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m     22\u001b[0m   \u001b[39m# Minibatches\u001b[39;00m\n\u001b[0;32m     23\u001b[0m   \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m):\n\u001b[1;32m---> 24\u001b[0m     train_step(training_input_tensor, training_output_tensor)\n\u001b[0;32m     26\u001b[0m   \u001b[39m# Print status\u001b[39;00m\n\u001b[0;32m     27\u001b[0m   now \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mnow()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training step\n",
    "@tf.function\n",
    "def train_step(input_data, output_data):\n",
    "    # Set up tape\n",
    "    with tf.GradientTape() as tape:\n",
    "      output = model(input_data, training=True)\n",
    "\n",
    "      loss = loss_function(output_data, output)\n",
    "\n",
    "      gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "      optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "# Training loop\n",
    "def train(epochs):\n",
    "  time_started = datetime.now()\n",
    "  example_time = 3\n",
    "  sculptures_made = 0\n",
    "\n",
    "  # Epochs\n",
    "  for epoch in range(epochs):\n",
    "    # Minibatches\n",
    "    for _ in range(1):\n",
    "      train_step(training_input_tensor, training_output_tensor)\n",
    "\n",
    "    # Print status\n",
    "    now = datetime.now()\n",
    "    # print(f\"Completed Epoch {epoch}\")\n",
    "\n",
    "    # Check if we should output an example sculpture\n",
    "    if (now-time_started).total_seconds() >= example_time:\n",
    "      example_time = int(example_time * 1.4)\n",
    "      print(\"Building example sculpture.\")\n",
    "      sculpture = build_sculpture(100)\n",
    "      sculptures_made += 1\n",
    "      sculpture_filename = f\"examples/json/example_{(time_started-datetime.utcfromtimestamp(0)).total_seconds()}_{sculptures_made}.json\"\n",
    "      save_sculpture(sculpture, sculpture_filename)\n",
    "      \n",
    "\n",
    "  \n",
    "train(1000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
