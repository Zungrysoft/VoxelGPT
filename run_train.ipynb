{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import random\n",
    "from datetime import datetime\n",
    "import json\n",
    "import training\n",
    "import color\n",
    "import vector as vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "# Network Structure\n",
    "CONTEXT_SIZE = 1      # How many other voxels are considered for a training example\n",
    "EMBEDDING_SIZE = 520    # Dimensionality of the voxel embedding vector\n",
    "STACKED_LAYERS = 1      # How many times the network structure repeats itself\n",
    "ATTENTION_HEADS = 10    # Number of heads in each multi-headed attention mechanism\n",
    "\n",
    "# Training Hyperparameters\n",
    "CHECK_RADIUS = 7        # How far away voxels can be to be part of a training example\n",
    "CENTER_FOCUS = 0.3      # How much to focus on picking voxels close to the center of the cube. Must be between 0 and 1.\n",
    "LEARNING_RATE = 1e-3\n",
    "TRAINING_EXAMPLES = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load voxel palette\n",
    "# The output is a 255-dimensional vector of probabilities for different colors\n",
    "# Which 255 colors can be generated is decided by the palette file\n",
    "\n",
    "# Index 0 is reserved as 'undecided' voxel\n",
    "# Index 1 is reserved as 'air' voxel\n",
    "# Index 2-255 are colors. So there are 254 possible colors.\n",
    "with open('data/palette.json', 'r') as json_file:\n",
    "    raw_palette = json.load(json_file)['colors']\n",
    "    palette = color.expand_palette(raw_palette)\n",
    "    palette_size = len(palette)\n",
    "\n",
    "print(f\"Palette has {palette_size} colors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def main_model():\n",
    "    input = keras.Input(shape=(CONTEXT_SIZE, EMBEDDING_SIZE,), name='input')\n",
    "    input_next_pos = keras.Input(shape=(CONTEXT_SIZE, EMBEDDING_SIZE,), name='input_next_pos')\n",
    "\n",
    "    # Normalization\n",
    "    x = keras.layers.LayerNormalization(name=f'normalization_start_a')(input)\n",
    "\n",
    "    # Stacked layers\n",
    "    for i in range(STACKED_LAYERS):\n",
    "        # Multi-headed attention\n",
    "        fx = keras.layers.MultiHeadAttention(\n",
    "            num_heads=ATTENTION_HEADS,\n",
    "            key_dim=EMBEDDING_SIZE,\n",
    "            name=f'multi_head_attention_{i}',\n",
    "        )(x, x, use_causal_mask=True)\n",
    "\n",
    "        # Residual connection\n",
    "        x = keras.layers.Add(name=f'residual_connection_{i}a')([x,fx])\n",
    "\n",
    "        # Normalization\n",
    "        x = keras.layers.LayerNormalization(name=f'normalization_{i}a')(x)\n",
    "\n",
    "        # Feedforward\n",
    "        fx = keras.layers.Dense(EMBEDDING_SIZE, name=f'feedforward_{i}')(x)\n",
    "        fx = keras.layers.LeakyReLU(name=f'relu_{i}')(fx)\n",
    "\n",
    "        # Residual connection\n",
    "        x = keras.layers.Add(name=f'residual_connection_{i}b')([x,fx])\n",
    "\n",
    "        # Normalization\n",
    "        x = keras.layers.LayerNormalization(name=f'normalization_{i}b')(x)\n",
    "    \n",
    "    # Concatenate with next_pos input\n",
    "    x = keras.layers.Concatenate(axis=2, name='concatenate_next_pos')([x,input_next_pos])\n",
    " \n",
    "    # Final feedforward layer\n",
    "    # Output size should be palette_size-1, since we don't want it to be able to choose \"undecided\"\n",
    "    x = keras.layers.Dense(palette_size-1, name='feedforward_final')(x)\n",
    "\n",
    "    # Softmax\n",
    "    x = keras.layers.Softmax(name='softmax')(x)\n",
    "    \n",
    "    # Build and return model\n",
    "    return keras.Model(inputs=[input, input_next_pos], outputs=x)\n",
    "\n",
    "model = main_model()\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up loss and optimizer\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design training examples\n",
    "training_examples = training.generate_training_examples(TRAINING_EXAMPLES, CONTEXT_SIZE)\n",
    "\n",
    "print(f\"{len(training_examples)} training examples created.\")\n",
    "\n",
    "print(random.choice(training_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_training_input(example):\n",
    "    inputEntry = []\n",
    "\n",
    "    # Encode context vector\n",
    "    for index, voxel in enumerate(example):\n",
    "        if len(inputEntry) < CONTEXT_SIZE+1:\n",
    "            inputEntry.append(training.embed(index, voxel[0], voxel[1], palette, EMBEDDING_SIZE))\n",
    "\n",
    "    # Pad remainder of context with zeros\n",
    "    if len(inputEntry) < CONTEXT_SIZE+1:\n",
    "        zero_elem = [0,] * EMBEDDING_SIZE\n",
    "        inputEntry += [zero_elem,] * ((CONTEXT_SIZE+1) - len(inputEntry))\n",
    "\n",
    "    return inputEntry\n",
    "\n",
    "def encode_training_input_next_pos(example, next_pos=None):\n",
    "    if next_pos == None:\n",
    "        inputEntry = []\n",
    "\n",
    "        # Encode context vector\n",
    "        for index, voxel in enumerate(example):\n",
    "            if len(inputEntry) < CONTEXT_SIZE+1:\n",
    "                inputEntry.append(training.embed(index, voxel[0], -1, palette, EMBEDDING_SIZE))\n",
    "\n",
    "        # Pad remainder of context with zeros\n",
    "        if len(inputEntry) < CONTEXT_SIZE+1:\n",
    "            zero_elem = [0,] * EMBEDDING_SIZE\n",
    "            inputEntry += [zero_elem,] * ((CONTEXT_SIZE+1) - len(inputEntry))\n",
    "        \n",
    "        return inputEntry\n",
    "    else:\n",
    "        # Create zero vector\n",
    "        zero_elem = [0,] * EMBEDDING_SIZE\n",
    "        inputEntry = [zero_elem,] * (CONTEXT_SIZE+1)\n",
    "\n",
    "        inputEntry[len(example)] = training.embed(len(example), next_pos, -1, palette, EMBEDDING_SIZE)\n",
    "\n",
    "        return inputEntry\n",
    "\n",
    "def encode_training_output(example):\n",
    "    outputEntry = []\n",
    "    for voxel in example:\n",
    "        outputEntry.append(training.encode_one_hot(voxel[1], palette_size))\n",
    "    return outputEntry\n",
    "\n",
    "# Reformat training examples into tensor format\n",
    "def encode_training_examples(training_examples):\n",
    "    training_inputs = []\n",
    "    training_inputs_next_pos = []\n",
    "    training_outputs = []\n",
    "    for example in training_examples:\n",
    "        input_entry = encode_training_input(example)\n",
    "        input_entry_next_pos = encode_training_input_next_pos(example)\n",
    "        output_entry = encode_training_output(example)\n",
    "        \n",
    "        # Shift the output\n",
    "        input_entry.pop(-1)\n",
    "        input_entry_next_pos.pop(0)\n",
    "        output_entry.pop(0)\n",
    "\n",
    "        # Push to training example list\n",
    "        training_inputs.append(input_entry)\n",
    "        training_inputs_next_pos.append(input_entry_next_pos)\n",
    "        training_outputs.append(output_entry)\n",
    "\n",
    "    training_input_tensor = tf.Variable(training_inputs, tf.float64)\n",
    "    training_input_next_pos_tensor = tf.Variable(training_inputs_next_pos, tf.float64)\n",
    "    training_output_tensor = tf.Variable(training_outputs, tf.float64)\n",
    "\n",
    "    return training_input_tensor, training_input_next_pos_tensor, training_output_tensor\n",
    "\n",
    "training_input_tensor, training_input_next_pos_tensor, training_output_tensor = encode_training_examples(training_examples)\n",
    "\n",
    "# print(training_input_tensor[0, :, :])\n",
    "# print(training_input_next_pos_tensor[0, :, :])\n",
    "# print(training_output_tensor[0, :, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch training data\n",
    "dataset_input = tf.data.Dataset.from_tensor_slices(training_input_tensor)\n",
    "dataset_input_next_pos = tf.data.Dataset.from_tensor_slices(training_input_next_pos_tensor)\n",
    "dataset_output = tf.data.Dataset.from_tensor_slices(training_output_tensor)\n",
    "\n",
    "training_input_batched = dataset_input.batch(128)\n",
    "training_input_next_pos_batched = dataset_input_next_pos.batch(128)\n",
    "training_output_batched = dataset_output.batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct an example sculpture using the model's current progress\n",
    "# TODO: Move this to a separate file so we can do multithreading and other improvements\n",
    "def build_sculpture(count, base=None, temperature=1.0):\n",
    "    # If a base wasn't specified, create one\n",
    "    color = int(random.random() * 254) + 1\n",
    "    voxels = {}\n",
    "    # start_pos = (0, 0, 0)\n",
    "    start_pos = (int(random.random() * training.SIZE[0]), int(random.random() * training.SIZE[1]), int(random.random() * training.SIZE[2]))\n",
    "    voxels[vec.ttos(start_pos)] = color\n",
    "    \n",
    "    # Build context vector for the sculpture\n",
    "    context = [(start_pos, color)]\n",
    "\n",
    "    for i in range(count-1):\n",
    "        # Determine where the next voxel will go\n",
    "        # TODO: Encode this data into the model somehow\n",
    "        next_pos = training.pick_next_voxel(voxels, context)\n",
    "\n",
    "        # Get the output from the model\n",
    "        input_data = encode_training_input(context)\n",
    "        input_data.pop(-1)\n",
    "        input_tensor = tf.Variable(input_data, tf.float64)\n",
    "        input_tensor = tf.reshape(input_tensor, [1, -1, EMBEDDING_SIZE])\n",
    "\n",
    "        input_next_pos_data = encode_training_input_next_pos(context, next_pos)\n",
    "        input_next_pos_data.pop(0)\n",
    "        input_next_pos_tensor = tf.Variable(input_next_pos_data, tf.float64)\n",
    "        input_next_pos_tensor = tf.reshape(input_tensor, [1, -1, EMBEDDING_SIZE])\n",
    "\n",
    "        output = model([input_tensor, input_next_pos_tensor], training=False)\n",
    "        output_probabilities = output[0][len(context)-1]\n",
    "\n",
    "        # Pick which voxel to generate based on output probabilities\n",
    "        # TODO: Implement temperature\n",
    "        choice = random.random()\n",
    "        chosen_voxel = 1\n",
    "        for i in range(len(output_probabilities)):\n",
    "            choice -= output_probabilities[i]\n",
    "            if choice < 0:\n",
    "                chosen_voxel = i+1\n",
    "                break\n",
    "        # chosen_voxel = 1\n",
    "        # best = 0\n",
    "        # for i in range(len(output_probabilities)):\n",
    "        #     if output_probabilities[i] > best:\n",
    "        #         best = output_probabilities[i]\n",
    "        #         chosen_voxel = i+1\n",
    "        \n",
    "        # Build the voxel\n",
    "        voxels[vec.ttos(next_pos)] = chosen_voxel\n",
    "        context.append((next_pos, chosen_voxel))\n",
    "        if len(context) > CONTEXT_SIZE:\n",
    "            context = training.remove_farthest(context, next_pos)\n",
    "        \n",
    "    return voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for saving sculptures to json\n",
    "def save_sculpture(sculpture, filename):\n",
    "    json_data = {\n",
    "        \"size\": {\n",
    "            \"x\": training.SIZE[0],\n",
    "            \"y\": training.SIZE[1],\n",
    "            \"z\": training.SIZE[2],\n",
    "        },\n",
    "        \"voxels\": sculpture,\n",
    "    }\n",
    "    with open(filename, 'w') as output_file:\n",
    "        json.dump(json_data, output_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step\n",
    "@tf.function\n",
    "def train_step(input_data, input_next_pos_data, output_data):\n",
    "    # Set up tape\n",
    "    with tf.GradientTape() as tape:\n",
    "      output = model([input_data, input_next_pos_data], training=True)\n",
    "\n",
    "      loss = loss_function(output_data, output)\n",
    "\n",
    "      gradients = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "      optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "# Training loop\n",
    "def train(epochs):\n",
    "  time_started = datetime.now()\n",
    "  example_time = 3\n",
    "  sculptures_made = 0\n",
    "\n",
    "  # Epochs\n",
    "  for epoch in range(epochs):\n",
    "    # Minibatches\n",
    "    for (batch_input, batch_input_next_pos, batch_output) in zip(training_input_batched, training_input_next_pos_batched, training_output_batched):\n",
    "      train_step(batch_input, batch_input_next_pos, batch_output)\n",
    "\n",
    "    # Print status\n",
    "    # print(f\"Completed Epoch {epoch}\")\n",
    "\n",
    "    # Check if we should output an example sculpture\n",
    "    if (datetime.now()-time_started).total_seconds() >= example_time:\n",
    "      print(f\"Epoch {epoch}\")\n",
    "      print(\"Building example sculpture...\")\n",
    "      # sculpture = build_sculpture(training.SIZE[0] * training.SIZE[1] * training.SIZE[2])\n",
    "      sculpture = build_sculpture(300)\n",
    "      sculptures_made += 1\n",
    "      sculpture_filename = f\"examples/json/example_{(time_started-datetime.utcfromtimestamp(0)).total_seconds()}_{sculptures_made}.json\"\n",
    "      save_sculpture(sculpture, sculpture_filename)\n",
    "      print(\"Done\")\n",
    "      example_time = (datetime.now()-time_started).total_seconds() * 1.3\n",
    "\n",
    "train(10000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sorpok.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
